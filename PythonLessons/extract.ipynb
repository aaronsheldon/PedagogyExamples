{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract for DATA3998\n",
    "\n",
    "## Notes\n",
    "This notebook is intended to transparently document the data extract preparation for the\n",
    "capstone projects. You do not need to rerun this notebook. Please use the `*.csv` files that\n",
    "were provided with this notebook.\n",
    "\n",
    "## Usage\n",
    "Carefully review each section in this notebook. Metadata descriptions and citations to the\n",
    "original source are provided for each table. In addition to the tables listed below three\n",
    "additional reference tables are provided.\n",
    "\n",
    "## Country\n",
    "The `Country.csv` file maps two character country codes to full names. The codes were parsed\n",
    "from metadata in the\n",
    "[Provider Enumeration System](https://www.nber.org/research/data/national-plan-and-provider-enumeration-system-nppes)\n",
    "available from the [National Bureau of Economic Research](https://www.nber.org/). This file\n",
    "contains less than 1000 records. The unique `Code` can be linked to the `Country` in the\n",
    "`Provider` table.\n",
    "\n",
    "### Record Layout\n",
    "|Field |Type     |Description                                                                              |\n",
    "|------|---------|-----------------------------------------------------------------------------------------|\n",
    "|`Code`|`CHAR(2)`|Country unique identifier. Leading characters are meaningful, including zeros and spaces.|\n",
    "|`Name`|`VARCHAR`|Full name of the country.                                                                |\n",
    "\n",
    "### Quality Control\n",
    "\n",
    "Regular Expression to find invalid records.\n",
    "\n",
    "```Re\n",
    "^(?!\"[A-Z]{2}\",\"[^\"]*\"$)\n",
    "```\n",
    "\n",
    "## State\n",
    "The `State.csv` file maps two character state codes to full names. The codes were parsed\n",
    "from metadata in the\n",
    "[Provider Enumeration System](https://www.nber.org/research/data/national-plan-and-provider-enumeration-system-nppes)\n",
    "available from the [National Bureau of Economic Research](https://www.nber.org/). This file\n",
    "contains less than 100 records. The unique `Code` can be linked to the `State` in the\n",
    "`Provider` table.\n",
    "\n",
    "### Record Layout\n",
    "|Field       |Type     |Description                                                                            |\n",
    "|------------|---------|---------------------------------------------------------------------------------------|\n",
    "|`Code`      |`CHAR(2)`|State unique identifier. Leading characters are meaningful, including zeros and spaces.|\n",
    "|`Name`      |`VARCHAR`|Full name of the state.                                                                |\n",
    "|`Governance`|`VARCHAR`|Jurisdictional governance. Values are `STATE` or `TERRITORY`.                          |\n",
    "\n",
    "### Quality Control\n",
    "\n",
    "Regular Expression to find invalid records.\n",
    "\n",
    "```Re\n",
    "^(?!\"[A-Z]{2}\",\"[^\"]*\",\"(STATE|TERRITORY)\"$)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Taxonomy\n",
    "The `Taxonomy.csv` file maps 10 character clinical specialty taxonomy codes to full\n",
    "descriptions. The file is provided as is from the\n",
    "[National Uniform Claim Committee](https://nucc.org/). This file contains less than 1000\n",
    "records and was processed in less than one second. The unique `Code` can be linked to the\n",
    "`Code` in the `Specialty` table. \n",
    "\n",
    "### Record Layout\n",
    "|Field           |Type      |Description                                                                                         |\n",
    "|----------------|----------|----------------------------------------------------------------------------------------------------|\n",
    "|`Code`          |`CHAR(10)`|Clinical Specialty unique identifier. Leading characters are meaningful, including zeros and spaces.|\n",
    "|`Grouping`      |`VARCHAR` |Highest level domain of clinical practice.                                                          |\n",
    "|`Classification`|`VARCHAR` |Discipline or field of clinical practice within the high level domain.                              |\n",
    "|`Specialization`|`VARCHAR` |Specialization within the discipline of clinical practice.                                          |\n",
    "|`Name`          |`VARCHAR` |Amalgamation of the domain, discipline, and specialization.                                         |\n",
    "|`Individual`    |`BOOLEAN` |Indicates whether the taxonomy category applies to individual practice or an organization.          |\n",
    "\n",
    "### Quality Control\n",
    "\n",
    "Regular Expression to find invalid records.\n",
    "\n",
    "```Re\n",
    "^(?!\"[A-Z0-9]{10}\",\"[^\"]*\",\"[^\"]*\",\"[^\"]*\",\"[^\"]*\",\"(True|False)\"$)\n",
    "```\n",
    "\n",
    "### Files\n",
    "* Source `Data\\nucc_taxonomy_240.csv`\n",
    "* Target `Data\\Clean\\Taxonomy.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Records: 874\r"
     ]
    }
   ],
   "source": [
    "# Usual suspects\n",
    "import pandas as pd\n",
    "from csv import QUOTE_ALL\n",
    "\n",
    "# Read\n",
    "taxonomysource = pd.read_csv(\"Data\\\\nucc_taxonomy_240.csv\", dtype = str)\n",
    "\n",
    "# Filter columns\n",
    "taxonomysource.replace(\"\\\"+\", \" \", regex = True)\n",
    "taxonomysource[\"Individual\"] = (taxonomysource[\"Section\"] == \"Individual\")\n",
    "taxonomysource.rename(\n",
    "    columns = { \"Display Name\": \"Name\" },\n",
    "    inplace = True\n",
    ")\n",
    "taxonomytarget = taxonomysource[\n",
    "    [\n",
    "        \"Code\",\n",
    "        \"Grouping\",\n",
    "        \"Classification\",\n",
    "        \"Specialization\",\n",
    "        \"Name\",\n",
    "        \"Individual\"\n",
    "    ]\n",
    "]\n",
    "\n",
    "# Write quoting all the string fields\n",
    "taxonomytarget.to_csv(\n",
    "    \"Data\\Clean\\Taxonomy.csv\",\n",
    "    index = False,\n",
    "    quoting = QUOTE_ALL\n",
    ")\n",
    "\n",
    "# Display complete\n",
    "print(f\"Records: {len(taxonomytarget.index)}\", end = \"\\r\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clinical Specialties\n",
    "\n",
    "The clinical specialties of the clinical providers were obtained from the\n",
    "[Provider Enumeration System](https://www.nber.org/research/data/national-plan-and-provider-enumeration-system-nppes)\n",
    "available from the [National Bureau of Economic Research](https://www.nber.org/). The file\n",
    "was filtered down to just the primary specialty of each clinical provider, extraneous fields\n",
    "were removed, and the columns were renamed to replace jargon and abbreviations with a\n",
    "concise human readable label. This file contains approximately 7000000 records and was\n",
    "processed within one minute using Python. The unique `Provider` field can be linked to the\n",
    "`Provider` field in the `Provider` table, and the `SourceProvider` and `TargetProvider` \n",
    "fields in `Followup` table.\n",
    "\n",
    "\n",
    "### Record Layout\n",
    "|Field     |Type      |Description                                                                                                               |\n",
    "|----------|----------|--------------------------------------------------------------------------------------------------------------------------|\n",
    "|`Provider`|`CHAR(10)`|Clinical provider's unique identifier. Leading characters are meaningful, including zeros and spaces.                     |\n",
    "|`Code`    |`CHAR(10)`|Taxonomy code of the clinical provider's primary specialty. Leading characters are meaningful, including zeros and spaces.|\n",
    "\n",
    "### Quality Control\n",
    "\n",
    "Regular Expression to find invalid records.\n",
    "\n",
    "```Re\n",
    "^(?!\"[0-9]{10}\",\"[A-Z0-9]{10}\"$)\n",
    "```\n",
    "\n",
    "### Files\n",
    "* Source `Data\\ptaxcode20235.csv`\n",
    "* Target `Data\\Clear\\Specialty.csv`\n",
    "\n",
    "Step through in blocks of 10000 appending to the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Records: 7502413\r"
     ]
    }
   ],
   "source": [
    "# Usual suspects\n",
    "import pandas as pd\n",
    "from csv import QUOTE_ALL\n",
    "\n",
    "# Clean up, rename, remove columns\n",
    "def cleanspecialty(source):\n",
    "    source.replace(\"\\\"+\", \" \", regex = True)\n",
    "    source.rename(\n",
    "        columns = {\n",
    "            \"npi\": \"Provider\",\n",
    "            \"ptaxcode\": \"Code\"\n",
    "        },\n",
    "        inplace = True\n",
    "    )\n",
    "    target = source[source[\"pprimtax\"] == \"Y\"][\n",
    "        [\n",
    "            \"Provider\",\n",
    "            \"Code\"\n",
    "        ]\n",
    "    ]\n",
    "    return target\n",
    "\n",
    "\n",
    "# Batch read\n",
    "chunksize = 10**4\n",
    "records = 0\n",
    "with pd.read_csv(\"Data\\ptaxcode20235.csv\", dtype = str, chunksize = chunksize) as specialtyreader:\n",
    "\n",
    "    # First chunk\n",
    "    specialtytarget = cleanspecialty(next(specialtyreader))\n",
    "\n",
    "    # Write with header\n",
    "    specialtytarget.to_csv(\n",
    "        \"Data\\Clean\\Specialty.csv\", \n",
    "        quoting = QUOTE_ALL,\n",
    "        index = False\n",
    "    )\n",
    "\n",
    "    # Progress\n",
    "    records += len(specialtytarget.index)\n",
    "    print(f\"Records: {records}\", end = \"\\r\")\n",
    "\n",
    "    # All other chunks\n",
    "    for specialtysource in specialtyreader:\n",
    "        specialtytarget = cleanspecialty(specialtysource)\n",
    "\n",
    "        # Append without header\n",
    "        specialtytarget.to_csv(\n",
    "            \"Data\\Clean\\Specialty.csv\",\n",
    "            mode = \"a\",\n",
    "            quoting = QUOTE_ALL,\n",
    "            header = False,\n",
    "            index = False\n",
    "        )\n",
    "\n",
    "        # Progress\n",
    "        records += len(specialtytarget.index)\n",
    "        print(f\"Records: {records}\", end = \"\\r\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Providers\n",
    "The addresses and practice types of clinical providers were obtained from the\n",
    "[Provider Enumeration System](https://www.nber.org/research/data/national-plan-and-provider-enumeration-system-nppes)\n",
    "available from the [National Bureau of Economic Research](https://www.nber.org/). Extraneous\n",
    "fields were removed, the address fields were coalesced to minimize missing values, the\n",
    "practice type was coerced into a Boolean indicator, and the columns were renamed to replace\n",
    "jargon and abbreviations with a concise human readable label. This file contains\n",
    "approximately 7000000 records and was processed within 10 minutes in Python. The unique\n",
    "`Provider` field can be linked to the `Provider` field in the `Specialty` table, and the\n",
    "`SourceProvider` and `TargetProvider` fields in `Followup` table.\n",
    "\n",
    "### Record Layout\n",
    "|Field       |Type      |Description                                                                                            |\n",
    "|------------|----------|-------------------------------------------------------------------------------------------------------|\n",
    "|`Provider`  |`CHAR(10)`|Clinical provider's unique identifier. Leading characters are meaningful, including zeros and spaces.  |\n",
    "|`Individual`|`BOOLEAN` |Indicator of individual practice. `True` indicates individual practice, `False` indicates organization.|\n",
    "|`Zip`       |`VARCHAR` |Postal Zip+4 code of the provider's practice location.                                                 |\n",
    "|`City`      |`VARCHAR` |City of the provider's practice location.                                                              |\n",
    "|`State`     |`CHAR(2)` |State of the provider's practice location. Maps to the `Code` in the `State.csv` file.                 |\n",
    "|`Country`   |`CHAR(2)` |Country of the provider's practice location. Maps to the `Code` in the `Country.csv` file.             |\n",
    "\n",
    "### Quality Control\n",
    "\n",
    "Regular Expression to find invalid records.\n",
    "\n",
    "```Re\n",
    "^(?!\"[0-9]{10}\",\"(True|False)\",\"[^\"]*\",\"[^\"]*\",\"([A-Z][A-Z.])?\",\"([A-Z]{2})?\"$)\n",
    "```\n",
    "\n",
    "### Files\n",
    "* Source `Data\\core20235.csv`\n",
    "* target `Data\\Clean\\Provider.csv`\n",
    "\n",
    "Step through in blocks of 10000 appending to the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Records: 7794261\r"
     ]
    }
   ],
   "source": [
    "# Usual suspects\n",
    "import pandas as pd\n",
    "from csv import QUOTE_ALL\n",
    "\n",
    "# Clean up, rename, remove columns\n",
    "def cleanproviders(source):\n",
    "    source.replace(\"\\\"+\", \" \", regex = True)\n",
    "    source.rename(columns = { \"npi\": \"Provider\" }, inplace = True)\n",
    "    source[\"Individual\"] = (source[\"entity\"] == \"1\")\n",
    "    source[\"Zip\"] = source[\"ploczip\"].where(pd.notnull, source[\"pmailzip\"])\n",
    "    source[\"City\"] = source[\"ploccityname\"].where(pd.notnull, source[\"pmailcityname\"])\n",
    "    source[\"State\"] = source[\"plocstatename\"].where(pd.notnull, source[\"pmailstatename\"])\n",
    "    source[\"Country\"] = source[\"ploccountry\"].where(pd.notnull, source[\"pmailcountry\"])\n",
    "    source[\"State\"] = source[\"State\"].str.replace(\n",
    "        \"((?<=[A-Z])[A-Z]+(?=[A-Z]))|([^A-Z]+)|((?<=[A-Z])[A-Z](?=[^A-Z]+[A-Z]))\",\n",
    "        \"\",\n",
    "        regex = True\n",
    "    ).str.replace(\n",
    "        \"(?<=^[A-Z])$\", \".\", regex = True\n",
    "    ).str.slice(start = 0, stop = 2)\n",
    "    source[\"Zip\"] = source[\"Zip\"].str.replace(\"(?<=^[0-9]{5})$\", \"0000\", regex = True)\n",
    "    target = source[\n",
    "        [\n",
    "            \"Provider\",\n",
    "            \"Individual\",\n",
    "            \"Zip\",\n",
    "            \"City\",\n",
    "            \"State\",\n",
    "            \"Country\"\n",
    "        ]\n",
    "    ]\n",
    "    return target\n",
    "\n",
    "# Batch read\n",
    "chunksize = 10**4\n",
    "records = 0\n",
    "with pd.read_csv(\"Data\\core20235.csv\", dtype = str, chunksize = chunksize) as providerreader:\n",
    "\n",
    "    # First chunk\n",
    "    providertarget = cleanproviders(next(providerreader))\n",
    "\n",
    "    # Write with header\n",
    "    providertarget.to_csv(\n",
    "        \"Data\\Clean\\Provider.csv\", \n",
    "        quoting = QUOTE_ALL,\n",
    "        index = False\n",
    "    )\n",
    "\n",
    "    # Progress\n",
    "    records += len(providertarget.index)\n",
    "    print(f\"Records: {records}\", end = \"\\r\")\n",
    "\n",
    "    # All other chunks\n",
    "    for providersource in providerreader:\n",
    "        providertarget = cleanproviders(providersource)\n",
    "\n",
    "        # Append without header\n",
    "        providertarget.to_csv(\n",
    "            \"Data\\Clean\\Provider.csv\",\n",
    "            mode = \"a\",\n",
    "            quoting = QUOTE_ALL,\n",
    "            header = False,\n",
    "            index = False\n",
    "        )\n",
    "\n",
    "        # Progress\n",
    "        records += len(providertarget.index)\n",
    "        print(f\"Records: {records}\", end = \"\\r\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Followups\n",
    "The records of followup encounters between pairs of clinical providers were obtained from\n",
    "the\n",
    "[Physician Shared Patients](https://www.nber.org/research/data/physician-shared-patient-patterns-data)\n",
    "available from the [National Bureau of Economic Research](https://www.nber.org/). Extraneous\n",
    "fields were removed, and the columns were renamed to replace jargon and abbreviations with a\n",
    "concise human readable label.\n",
    "\n",
    "Each file contains transactional data derived from a single year of Medicaid/Medicare use.\n",
    "A single record in the file corresponds to a unique pair of clinicians, and measures the\n",
    "total volume of followups with the followup clinical provider that occurred within a month\n",
    "of a clinical encounter with the initial clinical provider. In the course of a calendar year\n",
    "a client may have had multiple followup pairs of encounters. There are approximately\n",
    "55000000 records in each file, and each file was processed within 10 minutes in Python.\n",
    "\n",
    "The followup pairs of clinical providers are unique in this dataset. There is only one\n",
    "record for each followup pair. To be included in the dataset a followup pair of clinical\n",
    "providers must have had at least 11 clients in common in the calendar year of surveillance.\n",
    "The `SourceProvider` and `TargetProvider` fields can be linked to the `Provider` fields in\n",
    "the `Provider` and `Specialty` tables.\n",
    "\n",
    "### Record Layout\n",
    "|Field                   |Type      |Description                                                                                                                                    |\n",
    "|------------------------|----------|-----------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "|`Source Provider`       |`CHAR(10)`|Initial clinical provider's unique identifier. Leading characters are meaningful, including zeros and spaces.                                  |\n",
    "|`Target Provider`       |`CHAR(10)`|Followup clinical provider's unique identifier. Leading characters are meaningful, including zeros and spaces.                                 |\n",
    "|`Followups Within Month`|`INT`     |Number of pairs of encounters between the initial clinical provider and the followup clinical provider that occurred within a month each other.|\n",
    "|`Followups Within Day`  |`INT`     |Number of pairs of encounters between the initial clinical provider and the followup clinical provider that occurred on the same day.          |\n",
    "|`Unique Clients`        |`INT`     |Number of clients seen in the pairs of encounters. Always less than or equal to the number of pairs of encounters.                             |\n",
    "\n",
    "The three measure fields `Followups Within Month`, `Followups Within Day`, and\n",
    "`Unique Clients` take values between 0 and less than 100000. A handful of records will\n",
    "contain values exceeding 32000 and as such require the `INT` type, as `SMALLINT` is not\n",
    "large enough.\n",
    "\n",
    "### Quality Control\n",
    "\n",
    "Regular Expression to find invalid records.\n",
    "\n",
    "```Re\n",
    "^(?!\"[0-9]{10}\",\"[0-9]{10}\",\"[0-9]*\",\"[0-9]*\",\"[0-9]*\"$)\n",
    "```\n",
    "\n",
    "### Files\n",
    "There are seven to eight files for each calendar year, each containing no more than 7500000\n",
    "records.Each team will be assigned a single calendar year of data.\n",
    "* 2009 Calendar Year\n",
    "  * Source `Data\\pspp2009_30.csv`\n",
    "  * target `Data\\Clean\\Followup2009-{1-7}.csv`\n",
    "* 2010 Calendar Year\n",
    "  * Source `Data\\pspp2010_30.csv`\n",
    "  * target `Data\\Clean\\Followup2010-{1-7}.csv`\n",
    "* 2011 Calendar Year\n",
    "  * Source `Data\\pspp2011_30.csv`\n",
    "  * target `Data\\Clean\\Followup2011-{1-8}.csv`\n",
    "* 2012 Calendar Year\n",
    "  * Source `Data\\pspp2012_30.csv`\n",
    "  * target `Data\\Clean\\Followup2012-{1-8}.csv`\n",
    "\n",
    "Step through in blocks of 10000 appending to the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usual suspects\n",
    "import pandas as pd\n",
    "from csv import QUOTE_ALL\n",
    "\n",
    "# Clean up, rename, remove columns\n",
    "def cleanfollowups(source):\n",
    "    source.replace(\"\\\"+\", \" \", regex = True)\n",
    "    source.rename(\n",
    "        columns = {\n",
    "            \"npi1\": \"Source Provider\",\n",
    "            \"npi2\": \"Target Provider\",\n",
    "            \"paircount\": \"Followups Within Month\",\n",
    "            \"samedaycount\": \"Followups Within Day\",\n",
    "            \"benecount\": \"Unique Clients\"\n",
    "        },\n",
    "        inplace = True\n",
    "    )\n",
    "    target = source[\n",
    "        [\n",
    "            \"Source Provider\",\n",
    "            \"Target Provider\",\n",
    "            \"Followups Within Month\",\n",
    "            \"Followups Within Day\",\n",
    "            \"Unique Clients\"\n",
    "        ]\n",
    "    ]\n",
    "    return target\n",
    "\n",
    "# Batch read size\n",
    "chunksize = 10**4\n",
    "recordlimit = 75 * 10**5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2009 Calendar Year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Records: 50382951 Files: 7\r"
     ]
    }
   ],
   "source": [
    "# Batch read\n",
    "with pd.read_csv(\"Data\\pspp2009_30.csv\", dtype = str, chunksize = chunksize) as followupreader:\n",
    "    records = 0\n",
    "    files = 0\n",
    "    for followupsource in followupreader:\n",
    "        followuptarget = cleanfollowups(followupsource)\n",
    "\n",
    "        # Write with header\n",
    "        if records % recordlimit == 0:\n",
    "            files += 1\n",
    "            followuptarget.to_csv(\n",
    "                f\"Data\\Clean\\Followup2009-{files}.csv\",\n",
    "                quoting = QUOTE_ALL,\n",
    "                index = False\n",
    "            )\n",
    "        \n",
    "        # Append without header\n",
    "        else:\n",
    "            followuptarget.to_csv(\n",
    "                f\"Data\\Clean\\Followup2009-{files}.csv\",\n",
    "                mode = \"a\",\n",
    "                quoting = QUOTE_ALL,\n",
    "                header = False,\n",
    "                index = False\n",
    "            )\n",
    "        records += len(followuptarget.index)\n",
    "\n",
    "        # Progress\n",
    "        print(f\"Records: {records} Files: {files}\", end = \"\\r\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2010 Calendar Year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Records: 52236906 Files: 7\r"
     ]
    }
   ],
   "source": [
    "# Batch read\n",
    "with pd.read_csv(\"Data\\pspp2010_30.csv\", dtype = str, chunksize = chunksize) as followupreader:\n",
    "    records = 0\n",
    "    files = 0\n",
    "    for followupsource in followupreader:\n",
    "        followuptarget = cleanfollowups(followupsource)\n",
    "\n",
    "        # Write with header\n",
    "        if records % recordlimit == 0:\n",
    "            files += 1\n",
    "            followuptarget.to_csv(\n",
    "                f\"Data\\Clean\\Followup2010-{files}.csv\",\n",
    "                quoting = QUOTE_ALL,\n",
    "                index = False\n",
    "            )\n",
    "        \n",
    "        # Append without header\n",
    "        else:\n",
    "            followuptarget.to_csv(\n",
    "                f\"Data\\Clean\\Followup2010-{files}.csv\",\n",
    "                mode = \"a\",\n",
    "                quoting = QUOTE_ALL,\n",
    "                header = False,\n",
    "                index = False\n",
    "            )\n",
    "        records += len(followuptarget.index)\n",
    "\n",
    "        # Progress\n",
    "        print(f\"Records: {records} Files: {files}\", end = \"\\r\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2011 Calendar Year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Records: 54038549 Files: 8\r"
     ]
    }
   ],
   "source": [
    "# Batch read\n",
    "with pd.read_csv(\"Data\\pspp2011_30.csv\", dtype = str, chunksize = chunksize) as followupreader:\n",
    "    records = 0\n",
    "    files = 0\n",
    "    for followupsource in followupreader:\n",
    "        followuptarget = cleanfollowups(followupsource)\n",
    "\n",
    "        # Write with header\n",
    "        if records % recordlimit == 0:\n",
    "            files += 1\n",
    "            followuptarget.to_csv(\n",
    "                f\"Data\\Clean\\Followup2011-{files}.csv\",\n",
    "                quoting = QUOTE_ALL,\n",
    "                index = False\n",
    "            )\n",
    "        \n",
    "        # Append without header\n",
    "        else:\n",
    "            followuptarget.to_csv(\n",
    "                f\"Data\\Clean\\Followup2011-{files}.csv\",\n",
    "                mode = \"a\",\n",
    "                quoting = QUOTE_ALL,\n",
    "                header = False,\n",
    "                index = False\n",
    "            )\n",
    "        records += len(followuptarget.index)\n",
    "\n",
    "        # Progress\n",
    "        print(f\"Records: {records} Files: {files}\", end = \"\\r\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2012 Calendar Year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Records: 54966715 Files: 8\r"
     ]
    }
   ],
   "source": [
    "# Batch read\n",
    "with pd.read_csv(\"Data\\pspp2012_30.csv\", dtype = str, chunksize = chunksize) as followupreader:\n",
    "    records = 0\n",
    "    files = 0\n",
    "    for followupsource in followupreader:\n",
    "        followuptarget = cleanfollowups(followupsource)\n",
    "\n",
    "        # Write with header\n",
    "        if records % recordlimit == 0:\n",
    "            files += 1\n",
    "            followuptarget.to_csv(\n",
    "                f\"Data\\Clean\\Followup2012-{files}.csv\",\n",
    "                quoting = QUOTE_ALL,\n",
    "                index = False\n",
    "            )\n",
    "        \n",
    "        # Append without header\n",
    "        else:\n",
    "            followuptarget.to_csv(\n",
    "                f\"Data\\Clean\\Followup2012-{files}.csv\",\n",
    "                mode = \"a\",\n",
    "                quoting = QUOTE_ALL,\n",
    "                header = False,\n",
    "                index = False\n",
    "            )\n",
    "        records += len(followuptarget.index)\n",
    "\n",
    "        # Progress\n",
    "        print(f\"Records: {records} Files: {files}\", end = \"\\r\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
